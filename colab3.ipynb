{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f70452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49ca16ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers<4.58,>=4.56\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: datasets<3,>=2.20 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.21.0)\n",
      "Requirement already satisfied: datasets<3,>=2.20 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.21.0)\n",
      "Requirement already satisfied: accelerate>=0.34.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.34.2)\n",
      "Requirement already satisfied: accelerate>=0.34.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.34.2)\n",
      "Collecting accelerate>=0.34.2\n",
      "  Using cached accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting accelerate>=0.34.2\n",
      "  Using cached accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting peft<0.18,>=0.16\n",
      "  Using cached peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: trl in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.9.6)\n",
      "Collecting peft<0.18,>=0.16\n",
      "  Using cached peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: trl in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.9.6)\n",
      "Collecting trl\n",
      "  Using cached trl-0.25.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: sentencepiece in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.2.1)\n",
      "Collecting trl\n",
      "  Using cached trl-0.25.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: sentencepiece in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.2.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<4.58,>=4.56) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<4.58,>=4.56) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<4.58,>=4.56) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<4.58,>=4.56) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<4.58,>=4.56) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<4.58,>=4.56) (2025.9.18)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<4.58,>=4.56) (2.32.5)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<4.58,>=4.56) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<4.58,>=4.56) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<4.58,>=4.56) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<4.58,>=4.56) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<4.58,>=4.56) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<4.58,>=4.56) (2025.9.18)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<4.58,>=4.56) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<4.58,>=4.56)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<4.58,>=4.56) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<4.58,>=4.56) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets<3,>=2.20) (22.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets<3,>=2.20) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets<3,>=2.20) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets<3,>=2.20) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets<3,>=2.20) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<3,>=2.20) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets<3,>=2.20) (3.13.2)\n",
      "Requirement already satisfied: psutil in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from peft<0.18,>=0.16) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from peft<0.18,>=0.16) (2.7.1)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<4.58,>=4.56)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<4.58,>=4.56) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<4.58,>=4.56) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets<3,>=2.20) (22.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets<3,>=2.20) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets<3,>=2.20) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets<3,>=2.20) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets<3,>=2.20) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<3,>=2.20) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets<3,>=2.20) (3.13.2)\n",
      "Requirement already satisfied: psutil in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from peft<0.18,>=0.16) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from peft<0.18,>=0.16) (2.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/keerthana/Library/Python/3.10/lib/python/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers<4.58,>=4.56) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers<4.58,>=4.56) (1.2.0)\n",
      "INFO: pip is looking at multiple versions of trl to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting trl\n",
      "  Using cached trl-0.24.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached trl-0.23.1-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached trl-0.22.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/keerthana/Library/Python/3.10/lib/python/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers<4.58,>=4.56) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers<4.58,>=4.56) (1.2.0)\n",
      "INFO: pip is looking at multiple versions of trl to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting trl\n",
      "  Using cached trl-0.24.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached trl-0.23.1-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached trl-0.22.2-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached trl-0.22.1-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached trl-0.22.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached trl-0.21.0-py3-none-any.whl.metadata (11 kB)\n",
      "INFO: pip is still looking at multiple versions of trl to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached trl-0.20.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached trl-0.19.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached trl-0.19.0-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached trl-0.18.2-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached trl-0.18.1-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached trl-0.22.1-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached trl-0.22.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached trl-0.21.0-py3-none-any.whl.metadata (11 kB)\n",
      "INFO: pip is still looking at multiple versions of trl to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached trl-0.20.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached trl-0.19.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached trl-0.19.0-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached trl-0.18.2-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached trl-0.18.1-py3-none-any.whl.metadata (11 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached trl-0.18.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached trl-0.17.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Using cached trl-0.16.1-py3-none-any.whl.metadata (12 kB)\n",
      "  Using cached trl-0.16.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Using cached trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from trl) (14.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets<3,>=2.20) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets<3,>=2.20) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets<3,>=2.20) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets<3,>=2.20) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets<3,>=2.20) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets<3,>=2.20) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets<3,>=2.20) (0.4.1)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached trl-0.18.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached trl-0.17.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Using cached trl-0.16.1-py3-none-any.whl.metadata (12 kB)\n",
      "  Using cached trl-0.16.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Using cached trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from trl) (14.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets<3,>=2.20) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets<3,>=2.20) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets<3,>=2.20) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets<3,>=2.20) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets<3,>=2.20) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets<3,>=2.20) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets<3,>=2.20) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets<3,>=2.20) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets<3,>=2.20) (3.11)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers<4.58,>=4.56) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers<4.58,>=4.56) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers<4.58,>=4.56) (2025.10.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.13.0->peft<0.18,>=0.16) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.13.0->peft<0.18,>=0.16) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.13.0->peft<0.18,>=0.16) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft<0.18,>=0.16) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft<0.18,>=0.16) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/keerthana/Library/Python/3.10/lib/python/site-packages (from pandas->datasets<3,>=2.20) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->datasets<3,>=2.20) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->datasets<3,>=2.20) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/keerthana/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->datasets<3,>=2.20) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rich->trl) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/keerthana/Library/Python/3.10/lib/python/site-packages (from rich->trl) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets<3,>=2.20) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets<3,>=2.20) (3.11)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers<4.58,>=4.56) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers<4.58,>=4.56) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers<4.58,>=4.56) (2025.10.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.13.0->peft<0.18,>=0.16) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.13.0->peft<0.18,>=0.16) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.13.0->peft<0.18,>=0.16) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft<0.18,>=0.16) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft<0.18,>=0.16) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/keerthana/Library/Python/3.10/lib/python/site-packages (from pandas->datasets<3,>=2.20) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->datasets<3,>=2.20) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->datasets<3,>=2.20) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/keerthana/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->datasets<3,>=2.20) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rich->trl) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/keerthana/Library/Python/3.10/lib/python/site-packages (from rich->trl) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Using cached peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "Using cached accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Using cached trl-0.15.2-py3-none-any.whl (318 kB)\n",
      "Using cached peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "Using cached accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Using cached trl-0.15.2-py3-none-any.whl (318 kB)\n",
      "Installing collected packages: tokenizers, accelerate, transformers, peft, trl\n",
      "\u001b[2K  Attempting uninstall: tokenizers\n",
      "\u001b[2K    Found existing installation: tokenizers 0.19.1\n",
      "Installing collected packages: tokenizers, accelerate, transformers, peft, trl\n",
      "\u001b[2K  Attempting uninstall: tokenizers\n",
      "\u001b[2K    Found existing installation: tokenizers 0.19.1\n",
      "\u001b[2K    Uninstalling tokenizers-0.19.1:\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.19.1\n",
      "\u001b[2K  Attempting uninstall: accelerate\n",
      "\u001b[2K    Found existing installation: accelerate 0.34.2\n",
      "\u001b[2K    Uninstalling accelerate-0.34.2:\n",
      "\u001b[2K      Successfully uninstalled accelerate-0.34.2\n",
      "\u001b[2K    Uninstalling tokenizers-0.19.1:\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.19.1\n",
      "\u001b[2K  Attempting uninstall: accelerate\n",
      "\u001b[2K    Found existing installation: accelerate 0.34.2\n",
      "\u001b[2K    Uninstalling accelerate-0.34.2:\n",
      "\u001b[2K      Successfully uninstalled accelerate-0.34.2\n",
      "\u001b[2K  Attempting uninstall: transformers\n",
      "\u001b[2K    Found existing installation: transformers 4.44.2\n",
      "\u001b[2K  Attempting uninstall: transformers\n",
      "\u001b[2K    Found existing installation: transformers 4.44.2\n",
      "\u001b[2K    Uninstalling transformers-4.44.2:m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.44.2━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [transformers]\n",
      "\u001b[2K    Uninstalling transformers-4.44.2:0m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.44.2━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [transformers]\n",
      "\u001b[2K  Attempting uninstall: peft\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [transformers]\n",
      "\u001b[2K    Found existing installation: peft 0.11.1━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [transformers]\n",
      "\u001b[2K    Uninstalling peft-0.11.1:╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled peft-0.11.1━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [transformers]\n",
      "\u001b[2K  Attempting uninstall: peftm╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [transformers]\n",
      "\u001b[2K    Found existing installation: peft 0.11.1━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [transformers]\n",
      "\u001b[2K    Uninstalling peft-0.11.1:╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled peft-0.11.1━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [transformers]\n",
      "\u001b[2K  Attempting uninstall: trl━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [peft]ers]\n",
      "\u001b[2K    Found existing installation: trl 0.9.6\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [peft]\n",
      "\u001b[2K    Uninstalling trl-0.9.6:━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [peft]\n",
      "\u001b[2K      Successfully uninstalled trl-0.9.60m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [peft]\n",
      "\u001b[2K  Attempting uninstall: trl━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [peft]\n",
      "\u001b[2K    Found existing installation: trl 0.9.6\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [peft]\n",
      "\u001b[2K    Uninstalling trl-0.9.6:━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [peft]\n",
      "\u001b[2K      Successfully uninstalled trl-0.9.60m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [peft]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [trl]\u001b[32m4/5\u001b[0m [trl]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [trl]\u001b[32m4/5\u001b[0m [trl]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.4.0 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.57.1 which is incompatible.\n",
      "autogluon-timeseries 1.4.0 requires fugue>=0.9.0, but you have fugue 0.8.7 which is incompatible.\n",
      "autogluon-timeseries 1.4.0 requires statsforecast<2.0.2,>=1.7.0, but you have statsforecast 1.5.0 which is incompatible.\n",
      "autogluon-timeseries 1.4.0 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.57.1 which is incompatible.\n",
      "chromadb 0.5.23 requires tokenizers<=0.20.3,>=0.13.2, but you have tokenizers 0.22.1 which is incompatible.\n",
      "langchain-huggingface 1.0.0 requires langchain-core<2.0.0,>=1.0.0, but you have langchain-core 0.1.53 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.11.0 peft-0.17.1 tokenizers-0.22.1 transformers-4.57.1 trl-0.15.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.4.0 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.57.1 which is incompatible.\n",
      "autogluon-timeseries 1.4.0 requires fugue>=0.9.0, but you have fugue 0.8.7 which is incompatible.\n",
      "autogluon-timeseries 1.4.0 requires statsforecast<2.0.2,>=1.7.0, but you have statsforecast 1.5.0 which is incompatible.\n",
      "autogluon-timeseries 1.4.0 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.57.1 which is incompatible.\n",
      "chromadb 0.5.23 requires tokenizers<=0.20.3,>=0.13.2, but you have tokenizers 0.22.1 which is incompatible.\n",
      "langchain-huggingface 1.0.0 requires langchain-core<2.0.0,>=1.0.0, but you have langchain-core 0.1.53 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.11.0 peft-0.17.1 tokenizers-0.22.1 transformers-4.57.1 trl-0.15.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U \"transformers>=4.56,<4.58\" \"datasets>=2.20,<3\" \\\n",
    "  \"accelerate>=0.34.2\" \"peft>=0.16,<0.18\" trl sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cec9c525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps | dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"  # avoid fast-downloader dependency\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device_map = {\"\": \"mps\"}; dtype = torch.float16; dev = \"mps\"\n",
    "else:\n",
    "    device_map = {\"\": \"cpu\"}; dtype = torch.float32; dev = \"cpu\"\n",
    "\n",
    "print(\"Device:\", dev, \"| dtype:\", dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64846a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded to: /Users/keerthana/Keerthana/workspace/unsloth/_hf_cache_colab3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Disable HF progress bars to avoid traitlets/layout/contextvar errors\n",
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"  # keep the fast-downloader off on this kernel\n",
    "\n",
    "from huggingface_hub.utils import disable_progress_bars\n",
    "disable_progress_bars()\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "MODEL_ID  = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "CACHE_DIR = \"./_hf_cache_colab3\"\n",
    "\n",
    "local_model_path = snapshot_download(\n",
    "    repo_id=MODEL_ID,\n",
    "    local_dir=CACHE_DIR,\n",
    "    allow_patterns=[\"*.safetensors\",\"*.bin\",\"*.json\",\"*.model\",\"tokenizer*\",\"*merges*\"],\n",
    "    resume_download=True,\n",
    "    max_workers=8,\n",
    ")\n",
    "print(\"Downloaded to:\", local_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba9ff620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy on: mps:0 | ref on: CPU\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Policy (trainable via LoRA)\n",
    "policy = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=device_map,\n",
    "    attn_implementation=\"sdpa\",\n",
    ")\n",
    "policy.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Reference (frozen). Keep on CPU to save MPS memory.\n",
    "ref = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    torch_dtype=torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map={\"\": \"cpu\"},\n",
    "    attn_implementation=\"sdpa\",\n",
    ")\n",
    "ref.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(\"policy on:\", next(policy.parameters()).device, \"| ref on: CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b89099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Instruction: Provide a helpful, harmless, and honest reply.\\nAnswer:', 'chosen': 'His eyes are narrowed in concentration and he seems to be looking through me.\\n\\n\"What?\" I whisper.<|endoftext|>', 'rejected': '\"What the hell is going on with you?\" He asks the empty room.<|endoftext|>'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "pref = load_dataset(\"trl-lib/lm-human-preferences-sentiment\", split=\"train[:1000]\")\n",
    "\n",
    "def to_dpo(row):\n",
    "    prompt   = \"Instruction: \" + \"Provide a helpful, harmless, and honest reply.\\nAnswer:\"  # simple neutral prompt\n",
    "    chosen   = row[\"chosen\"].strip()   + tokenizer.eos_token\n",
    "    rejected = row[\"rejected\"].strip() + tokenizer.eos_token\n",
    "    return {\"prompt\": prompt, \"chosen\": chosen, \"rejected\": rejected}\n",
    "\n",
    "dpo_ds = pref.map(to_dpo, remove_columns=pref.column_names)\n",
    "print(dpo_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12f1f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DPOTrainer, DPOConfig\n",
    "from peft import LoraConfig\n",
    "\n",
    "peft_cfg = LoraConfig(\n",
    "    r=8, lora_alpha=16, lora_dropout=0.1,\n",
    "    bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\",\"gate_proj\"],\n",
    ")\n",
    "\n",
    "MAX_LEN = 384  # shorten on tight memory; try 320/256 if OOM\n",
    "\n",
    "dpo_cfg = DPOConfig(\n",
    "    output_dir=\"./colab3_smollm2_dpo\",\n",
    "    beta=0.1,                              # preference strength\n",
    "    per_device_train_batch_size=2,         # small batches\n",
    "    gradient_accumulation_steps=16,        # effective batch 32\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    save_total_limit=1,\n",
    "    optim=\"adafactor\",                     # memory-friendly\n",
    "    bf16=False, fp16=False,                # MPS uses fp16 internally—leave flags False\n",
    "    torch_compile=False,\n",
    "    max_length=MAX_LEN,\n",
    "    max_prompt_length=MAX_LEN//2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d1e4f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5g/fc_vylh55fn4gm_hcp9r140m0000gn/T/ipykernel_26218/810304074.py:5: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `DPOTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = DPOTrainer(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 02:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.692900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.689900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.690900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=32, training_loss=0.6907549723982811, metrics={'train_runtime': 164.0677, 'train_samples_per_second': 6.095, 'train_steps_per_second': 0.195, 'total_flos': 0.0, 'train_loss': 0.6907549723982811, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc, torch\n",
    "gc.collect()\n",
    "if torch.backends.mps.is_available(): torch.mps.empty_cache()\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=policy,\n",
    "    ref_model=None,                 # <-- let TRL clone a frozen ref internally\n",
    "    args=dpo_cfg,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dpo_ds,\n",
    "    peft_config=peft_cfg,\n",
    ")\n",
    "\n",
    "# reduce activation memory\n",
    "trainer.model.gradient_checkpointing_enable()\n",
    "trainer.model.enable_input_require_grads()\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "203692a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Prompt 1## \n",
      " This movie is amazing and I really loved it because it's such a beautiful, powerful, powerful film. I can't wait for more.\n",
      "\n",
      "Sunday, July 16, 2010\n",
      "\n",
      "How to Do a Good Job\n",
      "\n",
      "I'm writing this post because I'm really looking forward to the new film. It's a must see so I need to post this one before I go on my first trip to the movies.\n",
      "\n",
      "It's a movie about a man who's in a relationship with a woman who he's spending all his time with. It's a man who is the victim of a series of bad decisions. The woman he has an affair with is not only his wife, but a woman who he's spending all his time with. It's a man who's in a relationship with a woman who's a part of a far more dangerous relationship. What's worse is that this woman has been a victim of a man who was very nice to her.\n",
      "\n",
      "I'm not saying this is the worst film ever made. It's not. I'm just saying it's a movie that doesn't have a good ending for a reason.\n",
      "\n",
      "This movie is not as good as the movie I mentioned above, but it's better than any other movie I've seen that doesn\n",
      "##Prompt 2## \n",
      " This movie is shit and I really disliked it because it's so bad.\n",
      "\n",
      "It's a movie I saw in the theaters for the first time, and I was like, \"I'm like, it's too bad.\" I mean, it's like, \"I'm like, this is a great movie, but it's so bad.\"\n",
      "\n",
      "I have a feeling that there's a lot of people who like this movie, but there's a lot of people who don't.\n",
      "\n",
      "I want people to like it and I don't want to make it hard on people who don't like it, but I think you really need to go through the same thing as I did with the movie itself.\n",
      "\n",
      "I actually went back and I went through the same thing with the movie.\n",
      "\n",
      "I was like, \"I'm like, what the hell does this movie have to do with the movie?\" I mean, I knew it was a movie, but I was like, \"I don't know, this movie is so bad.\" I don't want to make it hard on people who don't like it, but I'm like, \"I know it's a movie, but I'm like, I don't want to make it hard on people who don't like it.\"\n",
      "\n",
      "I don\n",
      "##Prompt 2## \n",
      " This movie is shit and I really disliked it because it's so bad.\n",
      "\n",
      "It's a movie I saw in the theaters for the first time, and I was like, \"I'm like, it's too bad.\" I mean, it's like, \"I'm like, this is a great movie, but it's so bad.\"\n",
      "\n",
      "I have a feeling that there's a lot of people who like this movie, but there's a lot of people who don't.\n",
      "\n",
      "I want people to like it and I don't want to make it hard on people who don't like it, but I think you really need to go through the same thing as I did with the movie itself.\n",
      "\n",
      "I actually went back and I went through the same thing with the movie.\n",
      "\n",
      "I was like, \"I'm like, what the hell does this movie have to do with the movie?\" I mean, I knew it was a movie, but I was like, \"I don't know, this movie is so bad.\" I don't want to make it hard on people who don't like it, but I'm like, \"I know it's a movie, but I'm like, I don't want to make it hard on people who don't like it.\"\n",
      "\n",
      "I don\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=ref, tokenizer=tokenizer)\n",
    "\n",
    "prompt = \"This movie is amazing and I really loved it because\"\n",
    "generated_text = generator(prompt, max_length=50, num_return_sequences=1)[0]['generated_text']\n",
    "print(\"##Prompt 1## \\n\", generated_text)\n",
    "\n",
    "prompt = \"This movie is shit and I really disliked it because\"\n",
    "generated_text = generator(prompt, max_length=50, num_return_sequences=1)[0]['generated_text']\n",
    "print(\"##Prompt 2## \\n\", generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
